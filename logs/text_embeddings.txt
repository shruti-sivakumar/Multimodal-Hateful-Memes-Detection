(hateful-memes-venv) (base) shrutisivakumar@Shrutis-MacBook-Pro Facebook-Hateful-Memes-Challenge-2020 % python -m scripts.cache_text_embeddings

Caching bert - train
config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 541kB/s]
Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<00:00, 278kB/s]
vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 652kB/s]
tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 774kB/s]
model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 440M/440M [01:07<00:00, 6.55MB/s]
Loading weights: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 199/199 [00:00<00:00, 5106.52it/s, Materializing param=pooler.dense.weight]
BertModel LOAD REPORT from: bert-base-uncased
Key                                        | Status     | Details
-------------------------------------------+------------+--------
cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |        
cls.predictions.transform.dense.weight     | UNEXPECTED |        
cls.predictions.transform.dense.bias       | UNEXPECTED |        
cls.predictions.bias                       | UNEXPECTED |        
cls.seq_relationship.bias                  | UNEXPECTED |        
cls.seq_relationship.weight                | UNEXPECTED |        
cls.predictions.transform.LayerNorm.weight | UNEXPECTED |        

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 266/266 [00:33<00:00,  7.96it/s]

Caching distilbert - train
Loading weights: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4900.75it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]
DistilBertModel LOAD REPORT from: distilbert-base-uncased
Key                     | Status     | Details
------------------------+------------+--------
vocab_transform.weight  | UNEXPECTED |        
vocab_projector.bias    | UNEXPECTED |        
vocab_layer_norm.bias   | UNEXPECTED |        
vocab_transform.bias    | UNEXPECTED |        
vocab_layer_norm.weight | UNEXPECTED |        

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 266/266 [00:15<00:00, 16.65it/s]

Caching roberta - train
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 481/481 [00:00<00:00, 1.53MB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25.0/25.0 [00:00<00:00, 45.0kB/s]
vocab.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 1.67MB/s]
merges.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 805kB/s]
tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 2.12MB/s]
model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499M/499M [00:29<00:00, 16.7MB/s]
Loading weights: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 197/197 [00:00<00:00, 3980.78it/s, Materializing param=encoder.layer.11.output.dense.weight]
RobertaModel LOAD REPORT from: roberta-base
Key                             | Status     | Details
--------------------------------+------------+--------
lm_head.dense.bias              | UNEXPECTED |        
lm_head.bias                    | UNEXPECTED |        
roberta.embeddings.position_ids | UNEXPECTED |        
lm_head.layer_norm.bias         | UNEXPECTED |        
lm_head.layer_norm.weight       | UNEXPECTED |        
lm_head.dense.weight            | UNEXPECTED |        
pooler.dense.weight             | MISSING    |        
pooler.dense.bias               | MISSING    |        

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
- MISSING       :those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 266/266 [00:32<00:00,  8.11it/s]

Caching bert - dev
Loading weights: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 199/199 [00:00<00:00, 4012.09it/s, Materializing param=pooler.dense.weight]
BertModel LOAD REPORT from: bert-base-uncased
Key                                        | Status     | Details
-------------------------------------------+------------+--------
cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |        
cls.predictions.transform.dense.weight     | UNEXPECTED |        
cls.predictions.transform.dense.bias       | UNEXPECTED |        
cls.predictions.bias                       | UNEXPECTED |        
cls.seq_relationship.bias                  | UNEXPECTED |        
cls.seq_relationship.weight                | UNEXPECTED |        
cls.predictions.transform.LayerNorm.weight | UNEXPECTED |        

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00,  8.65it/s]

Caching distilbert - dev
Loading weights: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4146.29it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]
DistilBertModel LOAD REPORT from: distilbert-base-uncased
Key                     | Status     | Details
------------------------+------------+--------
vocab_transform.weight  | UNEXPECTED |        
vocab_projector.bias    | UNEXPECTED |        
vocab_layer_norm.bias   | UNEXPECTED |        
vocab_transform.bias    | UNEXPECTED |        
vocab_layer_norm.weight | UNEXPECTED |        

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 18.07it/s]

Caching roberta - dev
Loading weights: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 197/197 [00:00<00:00, 5014.25it/s, Materializing param=encoder.layer.11.output.dense.weight]
RobertaModel LOAD REPORT from: roberta-base
Key                             | Status     | Details
--------------------------------+------------+--------
lm_head.dense.bias              | UNEXPECTED |        
lm_head.bias                    | UNEXPECTED |        
roberta.embeddings.position_ids | UNEXPECTED |        
lm_head.layer_norm.bias         | UNEXPECTED |        
lm_head.layer_norm.weight       | UNEXPECTED |        
lm_head.dense.weight            | UNEXPECTED |        
pooler.dense.weight             | MISSING    |        
pooler.dense.bias               | MISSING    |        

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
- MISSING       :those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:02<00:00,  6.87it/s]

Caching bert - test
Loading weights: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 199/199 [00:00<00:00, 3672.12it/s, Materializing param=pooler.dense.weight]
BertModel LOAD REPORT from: bert-base-uncased
Key                                        | Status     | Details
-------------------------------------------+------------+--------
cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |        
cls.predictions.transform.dense.weight     | UNEXPECTED |        
cls.predictions.transform.dense.bias       | UNEXPECTED |        
cls.predictions.bias                       | UNEXPECTED |        
cls.seq_relationship.bias                  | UNEXPECTED |        
cls.seq_relationship.weight                | UNEXPECTED |        
cls.predictions.transform.LayerNorm.weight | UNEXPECTED |        

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.66it/s]

Caching distilbert - test
Loading weights: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4508.36it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]
DistilBertModel LOAD REPORT from: distilbert-base-uncased
Key                     | Status     | Details
------------------------+------------+--------
vocab_transform.weight  | UNEXPECTED |        
vocab_projector.bias    | UNEXPECTED |        
vocab_layer_norm.bias   | UNEXPECTED |        
vocab_transform.bias    | UNEXPECTED |        
vocab_layer_norm.weight | UNEXPECTED |        

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 19.00it/s]

Caching roberta - test
Loading weights: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 197/197 [00:00<00:00, 4587.48it/s, Materializing param=encoder.layer.11.output.dense.weight]
RobertaModel LOAD REPORT from: roberta-base
Key                             | Status     | Details
--------------------------------+------------+--------
lm_head.dense.bias              | UNEXPECTED |        
lm_head.bias                    | UNEXPECTED |        
roberta.embeddings.position_ids | UNEXPECTED |        
lm_head.layer_norm.bias         | UNEXPECTED |        
lm_head.layer_norm.weight       | UNEXPECTED |        
lm_head.dense.weight            | UNEXPECTED |        
pooler.dense.weight             | MISSING    |        
pooler.dense.bias               | MISSING    |        

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
- MISSING       :those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.93it/s]
Done.